---
title: "STAT 4620 Final Project"
author: "Phillip Brown, Ishan Gore, Vidhya Kewale, Connor McNeill, Nate Rezell"
date: "December 12, 2022"
output: html_document
---

The main question of interest for this project involves the Ames, Iowa housing dataset, and asks "How the features of a home add up to its price tag?" Our analysis involves forming a predictive model for the response variable, "SalePrice", as a function of the other 79 predictor variables contained in the dataset.

# Part I: Exploratory Data Analysis

Before we begin forming predictive models for the response variable, "SalePrice", as a function of the other 79 predictor variables in the Ames, Iowa housing dataset, we must complete some exploratory data analysis dealing with the data.

### Data Cleaning
``` {r results = "hide"}
train <- read.csv ("~/Desktop/Ames/train.csv", header = TRUE, row.names = 1, na.strings = ".")
test <- read.csv ("~/Desktop/Ames/test_new.csv", header = TRUE, row.names = 1, na.strings = ".")
```

``` {r}
dim (train)
dim (test)
```

Using the code above, we are able to find that the training dataset contains 1460 observations with 80 variables each. Likewise, the testing dataset contains 1447 observations, again with 80 variables each. The training dataset will allow us to create a predictive model for the response variable, "SalePrice", as a function of the other 79 predictor variables. The testing dataset will allow us to test the accuracy and precision of our model once created, and make a decision as to how well we are able to predict how the features of a home add up to its price tag.

``` {r results = "hide"}
str (train)
str (test)
```

Using the code above, we are able to see what kind of variables we have in our training and testing datasets. Before manipulating the datasets, we find that the data is comprised of two variable types: character variables and integer variables. With the exception of two predictor variables, these variable types remain constant between the training and testing datasets. The two variables that do not remain constant between the two datasets are "BsmtFullBath" and "BsmtHalfBath", both of which are included in the training dataset as integer variables, but in the testing dataset as character variables.

``` {r}
trainNA <- colSums (train == "NA")
which (trainNA > 0)

testNA <- colSums (test == "NA")
which (testNA > 0)
```

Using the code above, we can begin to see what will become one of our biggest issues that we will have to solve with data cleaning. Many of the predictor variables in both the training and testing datasets contain missing values (displayed as "NA"). We will begin to solve this issue in two ways below. For the creation of factor variables, we will simply create a component for the value of "NA". For the creation of integer variables, we will modify the value of "NA" to instead be represented by the integer 0.

The blocks of code below show the process of data cleaning for most of the predictor variables which we will use in the formation of our model to predict the response variable, "SalePrice". For variables that do not have continuous numerical values, we will create several components and modify the existing variable to become a factor variable instead. For variables that do have continuous numerical values, we will modify the existing variable to become an integer variable instead.

For all the variables modified to become integer variables, you will notice that we first must ensure that no missing values, which are displayed as "NA", remain, so we can ensure that the variable will no longer be considered a character variable. Therefore, before we convert the existing variables to integer variables, we modify the value of "NA" to instead be represented by the integer 0.

You will also notice that individual values are removed for some variables in the testing dataset, including the value of 150 in the "MSSubClass" variable, the values of "NA" in the "Utilities", "KitchenQual", and "Functional" variables. This is due to the fact that the training dataset does not contain those values for those respective variables, therefore it is impossible to use them to test the accuracy and precision of a predictive model, as the model was not trained to use them.

``` {r results = "hide"}
# MSSubClass
MSSubClass <- unique (union (train$MSSubClass, test$MSSubClass))
train$MSSubClass <- factor (train$MSSubClass, levels = MSSubClass)
test$MSSubClass <- factor (test$MSSubClass, levels = MSSubClass)

test <- test [!test$MSSubClass == 150, ]
```

``` {r results = "hide"}
# MSZoning
MSZoning <- unique (union (train$MSZoning, test$MSZoning))
train$MSZoning <- factor (train$MSZoning, levels = MSZoning)
test$MSZoning <- factor (test$MSZoning, levels = MSZoning)
```

``` {r results = "hide"}
# LotFrontage
train$LotFrontage [train$LotFrontage == "NA"] = 0
train$LotFrontage <- as.integer (train$LotFrontage)

test$LotFrontage [test$LotFrontage == "NA"] = 0
test$LotFrontage <- as.integer (test$LotFrontage)
```

``` {r results = "hide"}
# Street
Street <- unique (union (train$Street, test$Street))
train$Street <- factor (train$Street, levels = Street)
test$Street <- factor (test$Street, levels = Street)
```

``` {r results = "hide"}
# Alley
Alley <- unique (union (train$Alley, test$Alley))
train$Alley <- factor (train$Alley, levels = Alley)
test$Alley <- factor (test$Alley, levels = Alley)
```

``` {r results = "hide"}
# LotShape
LotShape <- unique (union (train$LotShape, test$LotShape))
train$LotShape <- factor (train$LotShape, levels = LotShape)
test$LotShape <- factor (test$LotShape, levels = LotShape)
```

``` {r results = "hide"}
# LandContour
LandContour <- unique (union (train$LandContour, test$LandContour))
train$LandContour <- factor (train$LandContour, levels = LandContour)
test$LandContour <- factor (test$LandContour, levels = LandContour)
```

``` {r results = "hide"}
# Utilities
Utilities <- unique (union (train$Utilities, test$Utilities))
train$Utilities <- factor (train$Utilities, levels = Utilities)
test$Utilities <- factor (test$Utilities, levels = Utilities)

test <- test [!test$Utilities == "NA", ]
```

``` {r results = "hide"}
# LotConfig
LotConfig <- unique (union (train$LotConfig, test$LotConfig))
train$LotConfig <- factor (train$LotConfig, levels = LotConfig)
test$LotConfig <- factor (test$LotConfig, levels = LotConfig)
```

``` {r results = "hide"}
# LandSlope
LandSlope <- unique (union (train$LandSlope, test$LandSlope))
train$LandSlope <- factor (train$LandSlope, levels = LandSlope)
test$LandSlope <- factor (test$LandSlope, levels = LandSlope)
```

``` {r results = "hide"}
# Neighborhood
Neighborhood <- unique (union (train$Neighborhood, test$Neighborhood))
train$Neighborhood <- factor (train$Neighborhood, levels = Neighborhood)
test$Neighborhood <- factor (test$Neighborhood, levels = Neighborhood)
```

``` {r results = "hide"}
# Condition1
Condition1 <- unique (union (train$Condition1, test$Condition1))
train$Condition1 <- factor (train$Condition1, levels = Condition1)
test$Condition1 <- factor (test$Condition1, levels = Condition1)
```

``` {r results = "hide"}
# Condition2
Condition2 <- unique (union (train$Condition2, test$Condition2))
train$Condition2 <- factor (train$Condition2, levels = Condition2)
test$Condition2 <- factor (test$Condition2, levels = Condition2)
```

``` {r results = "hide"}
# BldgType
BldgType <- unique (union (train$BldgType, test$BldgType))
train$BldgType <- factor (train$BldgType, levels = BldgType)
test$BldgType <- factor (test$BldgType, levels = BldgType)
```

``` {r results = "hide"}
# HouseStyle
HouseStyle <- unique (union (train$HouseStyle, test$HouseStyle))
train$HouseStyle <- factor (train$HouseStyle, levels = HouseStyle)
test$HouseStyle <- factor (test$HouseStyle, levels = HouseStyle)
```

``` {r results = "hide"}
# OverallQual
OverallQual <- unique (union (train$OverallQual, test$OverallQual))
train$OverallQual <- factor (train$OverallQual, levels = OverallQual)
test$OverallQual <- factor (test$OverallQual, levels = OverallQual)
```

``` {r results = "hide"}
# OverallCond
OverallCond <- unique (union (train$OverallCond, test$OverallCond))
train$OverallCond <- factor (train$OverallCond, levels = OverallCond)
test$OverallCond <- factor (test$OverallCond, levels = OverallCond)
```

``` {r results = "hide"}
# RoofStyle
RoofStyle <- unique (union (train$RoofStyle, test$RoofStyle))
train$RoofStyle <- factor (train$RoofStyle, levels = RoofStyle)
test$RoofStyle <- factor (test$RoofStyle, levels = RoofStyle)
```

``` {r results = "hide"}
# RoofMatl
RoofMatl <- unique (union (train$RoofMatl, test$RoofMatl))
train$RoofMatl <- factor (train$RoofMatl, levels = RoofMatl)
test$RoofMatl <- factor (test$RoofMatl, levels = RoofMatl)
```

``` {r results = "hide"}
# Exterior1st
Exterior1st <- unique (union (train$Exterior1st, test$Exterior1st))
train$Exterior1st <- factor (train$Exterior1st, levels = Exterior1st)
test$Exterior1st <- factor (test$Exterior1st, levels = Exterior1st)
```

``` {r results = "hide"}
# Exterior2nd
Exterior2nd <- unique (union (train$Exterior2nd, test$Exterior2nd))
train$Exterior2nd <- factor (train$Exterior2nd, levels = Exterior2nd)
test$Exterior2nd <- factor (test$Exterior2nd, levels = Exterior2nd)
```

``` {r results = "hide"}
# MasVnrType
MasVnrType <- unique (union (train$MasVnrType, test$MasVnrType))
train$MasVnrType <- factor (train$MasVnrType, levels = MasVnrType)
test$MasVnrType <- factor (test$MasVnrType, levels = MasVnrType)
```

``` {r results = "hide"}
# MasVnrArea
train$MasVnrArea [train$MasVnrArea == "NA"] = 0
train$MasVnrArea <- as.integer (train$MasVnrArea)

test$MasVnrArea [test$MasVnrArea == "NA"] = 0
test$MasVnrArea <- as.integer (test$MasVnrArea)
```

``` {r results = "hide"}
# ExterQual
ExterQual <- unique (union (train$ExterQual, test$ExterQual))
train$ExterQual <- factor (train$ExterQual, levels = ExterQual)
test$ExterQual <- factor (test$ExterQual, levels = ExterQual)
```

``` {r results = "hide"}
# ExterCond
ExterCond <- unique (union (train$ExterCond, test$ExterCond))
train$ExterCond <- factor (train$ExterCond, levels = ExterCond)
test$ExterCond <- factor (test$ExterCond, levels = ExterCond)
```

``` {r results = "hide"}
# Foundation
Foundation <- unique (union (train$Foundation, test$Foundation))
train$Foundation <- factor (train$Foundation, levels = Foundation)
test$Foundation <- factor (test$Foundation, levels = Foundation)
```

``` {r results = "hide"}
# BsmtQual
BsmtQual <- unique (union (train$BsmtQual, test$BsmtQual))
train$BsmtQual <- factor (train$BsmtQual, levels = BsmtQual)
test$BsmtQual <- factor (test$BsmtQual, levels = BsmtQual)
```

``` {r results = "hide"}
# BsmtCond
BsmtCond <- unique (union (train$BsmtCond, test$BsmtCond))
train$BsmtCond <- factor (train$BsmtCond, levels = BsmtCond)
test$BsmtCond <- factor (test$BsmtCond, levels = BsmtCond)
```

``` {r results = "hide"}
# BsmtExposure
BsmtExposure <- unique (union (train$BsmtExposure, test$BsmtExposure))
train$BsmtExposure <- factor (train$BsmtExposure, levels = BsmtExposure)
test$BsmtExposure <- factor (test$BsmtExposure, levels = BsmtExposure)
```

``` {r results = "hide"}
# BsmtFinType1
BsmtFinType1 <- unique (union (train$BsmtFinType1, test$BsmtFinType1))
train$BsmtFinType1 <- factor (train$BsmtFinType1, levels = BsmtFinType1)
test$BsmtFinType1 <- factor (test$BsmtFinType1, levels = BsmtFinType1)
```

``` {r results = "hide"}
# BsmtFinType2
BsmtFinType2 <- unique (union (train$BsmtFinType2, test$BsmtFinType2))
train$BsmtFinType2 <- factor (train$BsmtFinType2, levels = BsmtFinType2)
test$BsmtFinType2 <- factor (test$BsmtFinType2, levels = BsmtFinType2)
```

``` {r results = "hide"}
# Heating
Heating <- unique (union (train$Heating, test$Heating))
train$Heating <- factor (train$Heating, levels = Heating)
test$Heating <- factor (test$Heating, levels = Heating)
```

``` {r results = "hide"}
# HeatingQC
HeatingQC <- unique (union (train$HeatingQC, test$HeatingQC))
train$HeatingQC <- factor (train$HeatingQC, levels = HeatingQC)
test$HeatingQC <- factor (test$HeatingQC, levels = HeatingQC)
```

``` {r results = "hide"}
# CentralAir
CentralAir <- unique (union (train$CentralAir, test$CentralAir))
train$CentralAir <- factor (train$CentralAir, levels = CentralAir)
test$CentralAir <- factor (test$CentralAir, levels = CentralAir)
```

``` {r results = "hide"}
# Electrical
Electrical <- unique (union (train$Electrical, test$Electrical))
train$Electrical <- factor (train$Electrical, levels = Electrical)
test$Electrical <- factor (test$Electrical, levels = Electrical)
```

``` {r results = "hide"}
# BsmtFullBath
test$BsmtFullBath [test$BsmtFullBath == "NA"] = 0
test$BsmtFullBath <- as.integer (test$BsmtFullBath)
```

``` {r results = "hide"}
# BsmtHalfBath
test$BsmtHalfBath [test$BsmtHalfBath == "NA"] = 0
test$BsmtHalfBath <- as.integer (test$BsmtHalfBath)
```

``` {r results = "hide"}
# KitchenQual
KitchenQual <- unique (union (train$KitchenQual, test$KitchenQual))
train$KitchenQual <- factor (train$KitchenQual, levels = KitchenQual)
test$KitchenQual <- factor (test$KitchenQual, levels = KitchenQual)

test <- test [!test$KitchenQual == "NA", ]
```

``` {r results = "hide"}
# Functional
Functional <- unique (union (train$Functional, test$Functional))
train$Functional <- factor (train$Functional, levels = Functional)
test$Functional <- factor (test$Functional, levels = Functional)

test <- test [!test$Functional == "NA", ]
```

``` {r results = "hide"}
# FireplaceQu
FireplaceQu <- unique (union (train$FireplaceQu, test$FireplaceQu))
train$FireplaceQu <- factor (train$FireplaceQu, levels = FireplaceQu)
test$FireplaceQu <- factor (test$FireplaceQu, levels = FireplaceQu)
```

``` {r results = "hide"}
# GarageType
GarageType <- unique (union (train$GarageType, test$GarageType))
train$GarageType <- factor (train$GarageType, levels = GarageType)
test$GarageType <- factor (test$GarageType, levels = GarageType)
```

``` {r results = "hide"}
# GarageYrBlt
train$GarageYrBlt [train$GarageYrBlt == "NA"] = 0
train$GarageYrBlt <- as.integer (train$GarageYrBlt)

test$GarageYrBlt [test$GarageYrBlt == "NA"] = 0
test$GarageYrBlt <- as.integer (test$GarageYrBlt)
```

``` {r results = "hide"}
# GarageFinish
GarageFinish <- unique (union (train$GarageFinish, test$GarageFinish))
train$GarageFinish <- factor (train$GarageFinish, levels = GarageFinish)
test$GarageFinish <- factor (test$GarageFinish, levels = GarageFinish)
```

``` {r results = "hide"}
# GarageQual
GarageQual <- unique (union (train$GarageQual, test$GarageQual))
train$GarageQual <- factor (train$GarageQual, levels = GarageQual)
test$GarageQual <- factor (test$GarageQual, levels = GarageQual)
```

``` {r results = "hide"}
# GarageCond
GarageCond <- unique (union (train$GarageCond, test$GarageCond))
train$GarageCond <- factor (train$GarageCond, levels = GarageCond)
test$GarageCond <- factor (test$GarageCond, levels = GarageCond)
```

``` {r results = "hide"}
# PavedDrive
PavedDrive <- unique (union (train$PavedDrive, test$PavedDrive))
train$PavedDrive <- factor (train$PavedDrive, levels = PavedDrive)
test$PavedDrive <- factor (test$PavedDrive, levels = PavedDrive)
```

``` {r results = "hide"}
# PoolQC
PoolQC <- unique (union (train$PoolQC, test$PoolQC))
train$PoolQC <- factor (train$PoolQC, levels = PoolQC)
test$PoolQC <- factor (test$PoolQC, levels = PoolQC)
```

``` {r results = "hide"}
# Fence
Fence <- unique (union (train$Fence, test$Fence))
train$Fence <- factor (train$Fence, levels = Fence)
test$Fence <- factor (test$Fence, levels = Fence)
```

``` {r results = "hide"}
# MiscFeature
MiscFeature <- unique (union (train$MiscFeature, test$MiscFeature))
train$MiscFeature <- factor (train$MiscFeature, levels = MiscFeature)
test$MiscFeature <- factor (test$MiscFeature, levels = MiscFeature)
```

``` {r results = "hide"}
# SaleType
SaleType <- unique (union (train$SaleType, test$SaleType))
train$SaleType <- factor (train$SaleType, levels = SaleType)
test$SaleType <- factor (test$SaleType, levels = SaleType)

test <- test [!test$SaleType == "NA", ]
```

``` {r results = "hide"}
# SaleCondition
SaleCondition <- unique (union (train$SaleCondition, test$SaleCondition))
train$SaleCondition <- factor (train$SaleCondition, levels = SaleCondition)
test$SaleCondition <- factor (test$SaleCondition, levels = SaleCondition)
```

``` {r results = "hide"}
str (train)
str (test)
```

Using the code above, we can confirm after modifying and cleaning the training and testing datasets that the variables are still comprised of two variables types, but now those are factor variables and integer variables. We can also confirm that all of the variable types are constant between both the training and testing datasets. Therefore, the data from the 79 predictor variables is now usable for forming a predictive model for the response variable, "SalePrice".

### Data Exploration and Analysis
``` {r}
hist (train$SalePrice, xlab = "Sales Price", ylab = "Frequency", main = "Histogram of Sale Price from the Training Dataset")
summary (train$SalePrice)

hist (train$SalePrice, xlab = "Sales Price", ylab = "Frequency", main = "Histogram of Sale Price from the Testing Dataset")
summary (test$SalePrice)
```

Now that the data in both the training and testing datasets has been cleaned, we can begin to explore and analyze the data. Using the code above, we begin this process by studying our response variable in our model, "SalePrice". We find that the histograms for "SalePrice" from both the training and testing datasets are slightly skewed to the right, and the summary of the quantiles also indicates this fact. However, it is not a requirement that the reponse variable for the models be normally distributed, so we choose not to transform it using logarithms or square roots for added simplicity of interpretation.

``` {r}
train_int <- train [, unlist (lapply (train, is.integer))]
train_cor <- cor (train_int)
which (train_cor <= -0.5 | train_cor >= 0.5, arr.ind = TRUE)

test_int <- test [, unlist (lapply (train, is.integer))]
test_cor <- cor (test_int)
which (test_cor <= -0.5 | test_cor >= 0.5, arr.ind = TRUE)
```

Using the code above, we can see that many of the integer variables in both the training and testing datasets are striongly correlated with other variables. This could be an indication of potentially problematic collinearity amongst the predictor variables. Along with that, many of the variables that were strongly correlated with other variables were also strongly correlated with the response variable for our predictive model, "SalePrice". This could indicate that we should look for models that use shrinkage or dimension reduction to form predictions, as it would seem from the results above that not all of the predictor variables may be necessary. Another reason to consider models that use shrinkage or dimension reduction techniques to form predictions is because we began to see collinearity and strong correlation in only the integer variables without even beginning to consider all of the factor predictor variables. These factor variables will contain several components each, meaning that our model could have a large amount of predictors that could be unnecessary or simply repetetive of other variables.

Models that use shrinkage constrain the "size" of the estimated predictor variable coefficients towards 0. These models use a tuning parameter, $\lambda$, to control how much we want to shrink the regression coefficients. One model that uses this technique is the ridge regression model, which will be able to make some coefficients very small and nearly 0, but not 0 exactly. Another model that uses this technique is the LASSO model, which will perform variable selection by leading to some coefficients being set exactly to 0, and shrinking many other coefficients.

Models that use dimension reduction find transformed combinations of predictor variables, or components, to predict a response variable. In doing so, the model reduces the problems of estimating coefficients for all predictor variables, and instead only estimates coefficients for a significantly lower number of components. One model that uses this technique is the principal components linear regression model, which is unsupervised, meaning it performs dimension reduction of the predictor variables without any knowledge of the relationships between the predictor variables and the response variable. Another model that uses this technique is the linear regression model with components coming from partial least squares, which is supervised, and takes the relationship between the predictor variables and the response variable into account when performing dimension reduction.

# Part II: Model Analysis
``` {r message = FALSE, warning = FALSE}
library (glmnet)
library (pls)
```

### Linear Model using Least Squares
``` {r warning = FALSE}
fit.ls <- lm (SalePrice ~ ., train)
pred.ls <- predict (fit.ls, newdata = test)
mean ((test$SalePrice - pred.ls) ^ 2)
```

We begin our model analysis with the linear model using least squares. This model does not use any of the shrinkage or dimension reduction techniques we discussed above, therefore we will use it as a benchmark to see whether the shrinkage and dimension reduction models created below are more appropriate for predicting the response variable, "SalePrice". That being said, the mean squared error model with the testing dataset is `r mean ((test$SalePrice - pred.ls) ^ 2)`.

### Ridge Regression Model
``` {r}
xTrain <- model.matrix (SalePrice ~ ., train) [, -1]
yTrain = train$SalePrice

xTest <- model.matrix (SalePrice ~ ., test) [, -1]
yTest = test$SalePrice

set.seed (1)
ridge.cv = cv.glmnet (xTrain, yTrain, alpha = 0)
plot (ridge.cv)
lambda.cv = ridge.cv$lambda.min

fit.ridge <- glmnet (xTrain, yTrain, alpha = 0, lambda = lambda.cv)
pred.ridge <- predict (fit.ridge, newx = xTest)
mean ((yTest - pred.ridge) ^ 2)
```

The next model we attempted was the ridge regression model, the first of the two shrinkage models which we created. This model uses a tuning parameter, $\lambda$, here equal to a value of `r ridge.cv$lambda.min`, to shrink the regression coefficients of predictor variables to values very small and very near to 0, but not yet exactly 0. The mean squared error of the model with the testing set is `r mean ((yTest - pred.ridge) ^ 2)`. This is significantly lower that the linear model using least squares above, but we still may be able to get a better predictive model below.

### LASSO Model
``` {r}
set.seed (1)
lasso.cv <- cv.glmnet (xTrain, yTrain, alpha = 1)
plot (lasso.cv)
lambda.cv = lasso.cv$lambda.min

fit.lasso = glmnet (xTrain, yTrain, alpha = 1, lambda = lambda.cv)
pred.lasso <- predict (fit.lasso, newx = xTest)
mean ((yTest - pred.lasso) ^ 2)
```

The second of the models we attempted that use shrinkage was the LASSO model. Unlike the ridge regression model above, the tuning parameter here, equal to a value of `r lasso.cv$lambda.min`, will allow the shrinkage of regression coefficients to a value of 0. Therefore, this is the first model we have created that uses variable selection. The mean squared error using the testing dataset was `r mean ((yTest - pred.lasso) ^ 2)`, which is greater than the value for the ridge regression model above.

### Principal Component Linear Regression Model
``` {r}
fit.pcr <- pcr (SalePrice ~ ., data = train, scale  = FALSE, validation = "CV")
# summary (fit.pcr)
validationplot (fit.pcr, val.type = "MSEP")

fit.pcr2 <- pcr (SalePrice ~ ., data = test, scale = FALSE, ncomp = 160)
pcr.pred <- predict (fit.pcr2, ncomp = 160)

mean ((as.vector (pcr.pred) - test$SalePrice) ^ 2)
```

The next model we attempted was the principal component linear regression model, which is an unsupervised dimension reduction model, meaning it selects components made up of predictor variables without knowing anything about their relationship to the response variable. In this case, we began with 294 components, and the model was able to reduce it down to 160 components. Once reduced, the mean squared error of the testing dataset was `r mean ((as.vector (pcr.pred) - test$SalePrice) ^ 2)`, which makes this by far the most accurate model we have yet seen.

### Linear Regression Model with Components coming from Partial Least Squares
``` {r}
fit.pls <- plsr (SalePrice ~ ., data = train, scale = FALSE, validation = "CV")
# summary (fit.pls)
validationplot (fit.pls, val.type = "MSEP")

fit.pls2 <- plsr (SalePrice ~ ., data = test, scale = FALSE, ncomp = 28)
pls.pred <- predict (fit.pls2, ncomp = 28)

mean ((as.vector (pls.pred) - test$SalePrice) ^ 2)
```

The final model we used in an attempt to successfully predict the response variable "SalePrice" using the 79 other predictor variables in the Ames, Iowa housing dataset was the linear regression model with components coming from partial least squares. This model, like the principal component linear regression model above, used a dimension reduction technique, but a supervised one, meaning that it selected components with the knowledge of the relationship between the predictor variables and the response variable. Therefore, as the principal component linear model above was the best we had seen yet, we expected this one to be better, and therefore be the best predictive model overall. This model began with 294 components, and, once again, used dimension reduction to select reduce the number of components necessary to only 28. Using this number of components to create a predicitve model, the mean squared error of the testing dataset was `r mean ((as.vector (pls.pred) - test$SalePrice) ^ 2)`, which was indeed the lowest of any of the five models created.

### Visualizations for Assumptions of Normality of PLS Model

We chose the linear regression model with components coming from partial least squares as the best predictive model for the response variable "SalePrice" as a function of the 79 predictor variables. Not only did this model have the lowest mean squared error when run using the testing dataset, but it also completed the most variable selection by any of the models that used shrinkage or dimension reduction. Therefore, not only was the model highly accurate compared to the others, it was also significantly simpler and easy to use and interpret.

Now, we will use the residuals and the fitted values from the model after it was run using the testing dataset to create graphical summaries of the data. In doing so, we hope to find that the residuals of the model are normally distributed or approximately normally distributed, which would indicate that the model satisfies all modeling assumptions and is appropriate to be used to predict the response variable "SalePrice".

``` {r}
hist (fit.pls2$residuals, breaks = 100, xlab = "Residuals", ylab = "Frequency", main = "Histogram of Residuals")
```

When looking at a histogram of the residuals, the graph following the shape of a bell curve would indicate that it would be appropriate to assume normality. Because the residuals follow that shape in the histogram above, we can safely assume that the residuals are normally distributed and the model does not violate any assumptions.

``` {r}
qqnorm (fit.pls2$residuals)
qqline (fit.pls2$residuals)
```

When looking at a Q-Q plot, a straight line following the diagonal across the graph would indicate the it would be appropriate to assume normality of the residuals. When looking at our Q-Q plot above, most of our points in the middle of the line would indicate that the residuals follow a normal distribution, however the residuals seem to curve off of the line at the ends. Q-Q plots that make this shape are typically indicating that the data has more extreme values and outliers than would be expected in a true normal distribution. However, because most of the points fall along the line indicating normality, we can assume that the residuals are approximately normally distributed, and conclude that the model assumptions are satisfied.

``` {r}
plot (fit.pls2$fitted.values, fit.pls2$residuals, xlab = "Fitted Values", ylab = "Residuals", main = "Fitted Values vs. Residuals")
abline (h = 0)
```

Finally, when looking at the plot of fitted values vs. residuals, a random distribution of points centered around the value of 0 for the residuals without any sort of patterns would indicate both a normal distribution and a constant variance of the residuals. For the plot above of the residuals from our selected model, we can see that most of the points are randomly distributed around the center line of the graph, which displays where the residuals are equal to 0. However, as we say with the Q-Q plot above, the end of the graph appear to follow a pattern that would begin to indicate a violation of the assumptions for the residuals of the model. However, as with above, because this pattern only emerges at the very ends of the graph, it is simply suggesting that the data has more extreme values and outliers than would normally be present in a perfectly normal distribution. However, because this pattern only affects a few outliers, and most of the residuals appear to indicate approximate normality, we can conclude once again that the model assumptions are satisfied.

### Results

Using the Ames, Iowa housing training and testing datasets, we formed a predictive model for the response variable, "SalePrice", as a function of the other 79 predictor variables, to answer the question "How do the features of a home add up to its price tag?" The model we created was a linear regression model with components coming from partial least squares. This model is a supervised dimension reduction model, which means that it creates transformed combinations of the predictor variables, known as components, and selects the ones that are best able to predict a response variable. In our case, the model began with 294 components, and performed dimension reduction to reduce the number of components used in the model to 28, which is a reduction of over 90%. After using the training dataset to create this model, we tested it with a testing dataset, which resulted in a mean squared error of `r mean ((as.vector (pls.pred) - test$SalePrice) ^ 2)`, which was significantly less than any other models we tested. Therefore, due to the simplicity of the model in that it significantly reduced the number of necessary components, and the fact that it performed the best of any model in terms of mean squared error of the testing dataset, we choose the linear regression model with components coming from partial least squares as the best predictive model of the response variable "SalePrice".

We also created several visualizations for the assumptions that the model must satify to be considered appropriate and functional. All of the graphs we created using the residuals of the model and the testing dataset had similar issues in that there were extreme values and outliers present that would not be in a perfectly normal distribution. However, the three graphs still indicated that most of the data was approximately normally distributed. Therefore, we feel that it is appropriate to cautiously conclude that the assumptions of normality for the model are satisified, as long as future users of the model are cautious when using the model to predict extremely low or high values of "SalePrice".
